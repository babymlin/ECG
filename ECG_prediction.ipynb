{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ECG-prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babymlin/ECG/blob/main/ECG_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYvgQLZoaLsp"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, utils, callbacks, metrics\n",
        "import cv2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKy_HXz7aLsw",
        "outputId": "c2400a64-3fb3-4dc1-e0e6-3a60efe680c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "url = 'https://drive.google.com/file/d/1_OV0-zIKu4GlbDh_R1HBKeZx4fBO56Mn/view?usp=sharing'\n",
        "!gdown --id 1_OV0-zIKu4GlbDh_R1HBKeZx4fBO56Mn --output mitbth.csv\n",
        "df = pd.read_csv(\"mitbth.csv\", header=None)\n",
        "num_classes=5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_OV0-zIKu4GlbDh_R1HBKeZx4fBO56Mn\n",
            "To: /content/mitbth.csv\n",
            "\r0.00B [00:00, ?B/s]\r5.31MB [00:00, 46.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2vY5R1-aLsy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "13c877c1-a449-4433-8bff-9a6106ffe307"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.924866</td>\n",
              "      <td>0.992844</td>\n",
              "      <td>0.536673</td>\n",
              "      <td>0.182469</td>\n",
              "      <td>0.094812</td>\n",
              "      <td>0.051878</td>\n",
              "      <td>0.035778</td>\n",
              "      <td>0.060823</td>\n",
              "      <td>0.093023</td>\n",
              "      <td>0.187835</td>\n",
              "      <td>0.339893</td>\n",
              "      <td>0.427549</td>\n",
              "      <td>0.472272</td>\n",
              "      <td>0.474061</td>\n",
              "      <td>0.413238</td>\n",
              "      <td>0.470483</td>\n",
              "      <td>0.475850</td>\n",
              "      <td>0.468694</td>\n",
              "      <td>0.447227</td>\n",
              "      <td>0.447227</td>\n",
              "      <td>0.447227</td>\n",
              "      <td>0.431127</td>\n",
              "      <td>0.398927</td>\n",
              "      <td>0.397138</td>\n",
              "      <td>0.415027</td>\n",
              "      <td>0.389982</td>\n",
              "      <td>0.350626</td>\n",
              "      <td>0.364937</td>\n",
              "      <td>0.381038</td>\n",
              "      <td>0.381038</td>\n",
              "      <td>0.363148</td>\n",
              "      <td>0.388193</td>\n",
              "      <td>0.407871</td>\n",
              "      <td>0.438283</td>\n",
              "      <td>0.454383</td>\n",
              "      <td>0.479428</td>\n",
              "      <td>0.474061</td>\n",
              "      <td>0.499106</td>\n",
              "      <td>0.520572</td>\n",
              "      <td>0.513417</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.981324</td>\n",
              "      <td>0.964346</td>\n",
              "      <td>0.414261</td>\n",
              "      <td>0.005093</td>\n",
              "      <td>0.139219</td>\n",
              "      <td>0.293718</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.288625</td>\n",
              "      <td>0.280136</td>\n",
              "      <td>0.276740</td>\n",
              "      <td>0.283531</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>0.283531</td>\n",
              "      <td>0.280136</td>\n",
              "      <td>0.281834</td>\n",
              "      <td>0.292020</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>0.285229</td>\n",
              "      <td>0.292020</td>\n",
              "      <td>0.298812</td>\n",
              "      <td>0.298812</td>\n",
              "      <td>0.293718</td>\n",
              "      <td>0.292020</td>\n",
              "      <td>0.305603</td>\n",
              "      <td>0.300509</td>\n",
              "      <td>0.295416</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.303905</td>\n",
              "      <td>0.308998</td>\n",
              "      <td>0.315789</td>\n",
              "      <td>0.315789</td>\n",
              "      <td>0.317487</td>\n",
              "      <td>0.319185</td>\n",
              "      <td>0.331070</td>\n",
              "      <td>0.336163</td>\n",
              "      <td>0.336163</td>\n",
              "      <td>0.339559</td>\n",
              "      <td>0.346350</td>\n",
              "      <td>...</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.295416</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.985486</td>\n",
              "      <td>0.804064</td>\n",
              "      <td>0.438316</td>\n",
              "      <td>0.158200</td>\n",
              "      <td>0.014514</td>\n",
              "      <td>0.074020</td>\n",
              "      <td>0.121916</td>\n",
              "      <td>0.088534</td>\n",
              "      <td>0.078374</td>\n",
              "      <td>0.092888</td>\n",
              "      <td>0.082729</td>\n",
              "      <td>0.068215</td>\n",
              "      <td>0.071118</td>\n",
              "      <td>0.075472</td>\n",
              "      <td>0.082729</td>\n",
              "      <td>0.088534</td>\n",
              "      <td>0.088534</td>\n",
              "      <td>0.100145</td>\n",
              "      <td>0.121916</td>\n",
              "      <td>0.133527</td>\n",
              "      <td>0.169811</td>\n",
              "      <td>0.197388</td>\n",
              "      <td>0.220610</td>\n",
              "      <td>0.252540</td>\n",
              "      <td>0.283019</td>\n",
              "      <td>0.325109</td>\n",
              "      <td>0.346880</td>\n",
              "      <td>0.361393</td>\n",
              "      <td>0.383164</td>\n",
              "      <td>0.397678</td>\n",
              "      <td>0.403483</td>\n",
              "      <td>0.396226</td>\n",
              "      <td>0.375907</td>\n",
              "      <td>0.370102</td>\n",
              "      <td>0.349782</td>\n",
              "      <td>0.330914</td>\n",
              "      <td>0.320755</td>\n",
              "      <td>0.297533</td>\n",
              "      <td>0.283019</td>\n",
              "      <td>0.264151</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.827320</td>\n",
              "      <td>0.755155</td>\n",
              "      <td>0.520619</td>\n",
              "      <td>0.271907</td>\n",
              "      <td>0.123711</td>\n",
              "      <td>0.059278</td>\n",
              "      <td>0.030928</td>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039948</td>\n",
              "      <td>0.144330</td>\n",
              "      <td>0.244845</td>\n",
              "      <td>0.291237</td>\n",
              "      <td>0.311856</td>\n",
              "      <td>0.322165</td>\n",
              "      <td>0.362113</td>\n",
              "      <td>0.407216</td>\n",
              "      <td>0.442010</td>\n",
              "      <td>0.475515</td>\n",
              "      <td>0.501289</td>\n",
              "      <td>0.518041</td>\n",
              "      <td>0.530928</td>\n",
              "      <td>0.559278</td>\n",
              "      <td>0.590206</td>\n",
              "      <td>0.606959</td>\n",
              "      <td>0.644330</td>\n",
              "      <td>0.657216</td>\n",
              "      <td>0.690722</td>\n",
              "      <td>0.713918</td>\n",
              "      <td>0.747423</td>\n",
              "      <td>0.764175</td>\n",
              "      <td>0.786082</td>\n",
              "      <td>0.789948</td>\n",
              "      <td>0.798969</td>\n",
              "      <td>0.797680</td>\n",
              "      <td>0.788660</td>\n",
              "      <td>0.771907</td>\n",
              "      <td>0.759021</td>\n",
              "      <td>0.722938</td>\n",
              "      <td>0.695876</td>\n",
              "      <td>...</td>\n",
              "      <td>0.862113</td>\n",
              "      <td>0.608247</td>\n",
              "      <td>0.568299</td>\n",
              "      <td>0.565722</td>\n",
              "      <td>0.534794</td>\n",
              "      <td>0.502577</td>\n",
              "      <td>0.48067</td>\n",
              "      <td>0.45232</td>\n",
              "      <td>0.426546</td>\n",
              "      <td>0.417526</td>\n",
              "      <td>0.399485</td>\n",
              "      <td>0.393041</td>\n",
              "      <td>0.390464</td>\n",
              "      <td>0.385309</td>\n",
              "      <td>0.382732</td>\n",
              "      <td>0.386598</td>\n",
              "      <td>0.382732</td>\n",
              "      <td>0.378866</td>\n",
              "      <td>0.376289</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.965517</td>\n",
              "      <td>0.620690</td>\n",
              "      <td>0.206897</td>\n",
              "      <td>0.187739</td>\n",
              "      <td>0.118774</td>\n",
              "      <td>0.009579</td>\n",
              "      <td>0.005747</td>\n",
              "      <td>0.095785</td>\n",
              "      <td>0.203065</td>\n",
              "      <td>0.258621</td>\n",
              "      <td>0.268199</td>\n",
              "      <td>0.273946</td>\n",
              "      <td>0.289272</td>\n",
              "      <td>0.293103</td>\n",
              "      <td>0.293103</td>\n",
              "      <td>0.302682</td>\n",
              "      <td>0.312261</td>\n",
              "      <td>0.312261</td>\n",
              "      <td>0.308429</td>\n",
              "      <td>0.316092</td>\n",
              "      <td>0.325671</td>\n",
              "      <td>0.329502</td>\n",
              "      <td>0.329502</td>\n",
              "      <td>0.340996</td>\n",
              "      <td>0.354406</td>\n",
              "      <td>0.356322</td>\n",
              "      <td>0.360153</td>\n",
              "      <td>0.371648</td>\n",
              "      <td>0.385057</td>\n",
              "      <td>0.394636</td>\n",
              "      <td>0.400383</td>\n",
              "      <td>0.421456</td>\n",
              "      <td>0.434866</td>\n",
              "      <td>0.452107</td>\n",
              "      <td>0.459770</td>\n",
              "      <td>0.471264</td>\n",
              "      <td>0.484674</td>\n",
              "      <td>0.492337</td>\n",
              "      <td>0.494253</td>\n",
              "      <td>0.503831</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 188 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4    ...  183  184  185  186  187\n",
              "0  0.924866  0.992844  0.536673  0.182469  0.094812  ...  0.0  0.0  0.0  0.0  0.0\n",
              "1  0.981324  0.964346  0.414261  0.005093  0.139219  ...  0.0  0.0  0.0  0.0  0.0\n",
              "2  0.985486  0.804064  0.438316  0.158200  0.014514  ...  0.0  0.0  0.0  0.0  0.0\n",
              "3  0.827320  0.755155  0.520619  0.271907  0.123711  ...  0.0  0.0  0.0  0.0  2.0\n",
              "4  0.965517  0.620690  0.206897  0.187739  0.118774  ...  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 188 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBJn-xmSbFQ4"
      },
      "source": [
        "df_train, df_val = train_test_split(df, test_size=0.2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0sSv4J1aLtN"
      },
      "source": [
        "x_train = df_train.iloc[:, :-1]\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "x_val = df_val.iloc[:, :-1]\n",
        "x_val = np.expand_dims(x_val, axis=-1)\n",
        "y_val = df_val.iloc[:, -1]\n",
        "\n",
        "y_train = utils.to_categorical(y_train, num_classes=num_classes)\n",
        "y_val = utils.to_categorical(y_val, num_classes=num_classes)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5YZZJWZaLtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c8933d-3003-43ea-c520-ff5958ad87b3"
      },
      "source": [
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1751, 187, 1), (1751, 5), (438, 187, 1), (438, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hfp2IZWofSm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "92febdf1-853b-4157-d334-73355463de8f"
      },
      "source": [
        "idx = 18\n",
        "print('label: ', y_val[idx])\n",
        "plt.plot(x_val[idx])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label:  [1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2485770c10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AlqHd5bow0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e02e147-10eb-4bb7-8c95-7af875d88019"
      },
      "source": [
        "np.unique(np.argmax(y_train, axis=-1), return_counts=True), np.unique(np.argmax(y_val, axis=-1), return_counts=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([0, 1, 2, 3, 4]), array([1453,   52,  113,   15,  118])),\n",
              " (array([0, 1, 2, 3, 4]), array([369,  10,  22,   4,  33])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Git1WfBWaLtT"
      },
      "source": [
        "def build_model():\n",
        "    inputs = layers.Input(shape=x_train.shape[1:])\n",
        "    conv1_1 = layers.Conv1D(64, (6), activation='relu')(inputs)\n",
        "    conv1_1 = layers.BatchNormalization()(conv1_1)\n",
        "    pool1 = layers.Conv1D(64, 3, strides=(2), padding=\"same\")(conv1_1)\n",
        "    conv2_1 = layers.Conv1D(128, (3), activation='relu')(pool1)\n",
        "    conv2_1 = layers.BatchNormalization()(conv2_1)\n",
        "    pool2 = layers.Conv1D(128, 3, strides=(2), padding=\"same\")(conv2_1)\n",
        "    conv3_1 = layers.Conv1D(256, (3), activation='relu')(pool2)\n",
        "    conv3_1 = layers.BatchNormalization()(conv3_1)\n",
        "    pool3 = layers.Conv1D(256, 3, strides=(2), padding=\"same\")(conv3_1)\n",
        "    pool3 = layers.MaxPooling1D()(pool3)\n",
        "    flatten = layers.GlobalAveragePooling1D()(pool3)\n",
        "    dense_end1 = layers.Dense(64, activation='relu')(flatten)\n",
        "    dense_end2 = layers.Dense(32, activation='relu')(dense_end1)\n",
        "    main_output = layers.Dense(num_classes, activation='softmax', name='main_output')(dense_end2)\n",
        "    \n",
        "    model = models.Model(inputs, main_output)\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFNobbp5aLtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2876ea95-4bef-4c70-cfa9-24e3b7367c60"
      },
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 187, 1)]          0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 182, 64)           448       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 182, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 91, 64)            12352     \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 89, 128)           24704     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 89, 128)           512       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 45, 128)           49280     \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 43, 256)           98560     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 43, 256)           1024      \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 22, 256)           196864    \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 11, 256)           0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "main_output (Dense)          (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 402,693\n",
            "Trainable params: 401,797\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZNJNwYJaLtZ"
      },
      "source": [
        "callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
        "            #tf.keras.callbacks.ModelCheckpoint(filepath='best_imbalanced_model.h5', monitor='val_loss', save_best_only=True),\n",
        "            tf.keras.callbacks.ModelCheckpoint(filepath='best_balanced_model.h5', monitor='val_loss', save_best_only=True)\n",
        "            ]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXxW_9qV8vQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c8fdcb-76b0-4860-cd22-8d4e0dc25a89"
      },
      "source": [
        "# Calculate Class Weight\n",
        "data_count = np.unique(np.argmax(y_train, axis=-1), return_counts=True)[1]\n",
        "data_count\n",
        "weights = (1/data_count)*np.sum(data_count)/num_classes\n",
        "class_weight = {i: w for i, w in enumerate(weights)}\n",
        "print('class_weight', class_weight)\n",
        "# class_weight {0: 0.23920765027322405, 1: 6.607547169811321, 2: 3.4333333333333336, 3: 23.346666666666668, 4: 2.9931623931623936}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class_weight {0: 0.24101858224363384, 1: 6.734615384615386, 2: 3.0991150442477875, 3: 23.346666666666668, 4: 2.9677966101694913}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIHFGNEXaLtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd1345f-ab3d-4043-e221-5265e1c3983c"
      },
      "source": [
        "history = model.fit(\n",
        "    x_train, \n",
        "    y_train, \n",
        "    epochs=1000,\n",
        "    callbacks=callback, \n",
        "    batch_size=32,\n",
        "    validation_data=(x_val, y_val),\n",
        "    # Weighted Loss function\n",
        "    class_weight={0: 0.23920765027322405, 1: 6.607547169811321, 2: 3.4333333333333336, 3: 23.346666666666668, 4: 2.9931623931623936}\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "Epoch 1/1000\n",
            "55/55 [==============================] - 5s 26ms/step - loss: 1.5505 - accuracy: 0.3507 - val_loss: 1.7026 - val_accuracy: 0.0228\n",
            "Epoch 2/1000\n",
            "55/55 [==============================] - 0s 9ms/step - loss: 1.2178 - accuracy: 0.4563 - val_loss: 2.3400 - val_accuracy: 0.0228\n",
            "Epoch 3/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.8852 - accuracy: 0.5751 - val_loss: 3.9840 - val_accuracy: 0.0228\n",
            "Epoch 4/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.6690 - accuracy: 0.5951 - val_loss: 5.3087 - val_accuracy: 0.0228\n",
            "Epoch 5/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.6359 - accuracy: 0.6385 - val_loss: 7.0035 - val_accuracy: 0.0228\n",
            "Epoch 6/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.4596 - accuracy: 0.7059 - val_loss: 8.1509 - val_accuracy: 0.0228\n",
            "Epoch 7/1000\n",
            "55/55 [==============================] - 0s 9ms/step - loss: 0.4760 - accuracy: 0.7424 - val_loss: 5.4986 - val_accuracy: 0.0228\n",
            "Epoch 8/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.5335 - accuracy: 0.6368 - val_loss: 6.2432 - val_accuracy: 0.0274\n",
            "Epoch 9/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.5264 - accuracy: 0.7236 - val_loss: 3.5869 - val_accuracy: 0.0525\n",
            "Epoch 10/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.4002 - accuracy: 0.7447 - val_loss: 1.9779 - val_accuracy: 0.2123\n",
            "Epoch 11/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.3149 - accuracy: 0.7841 - val_loss: 3.0133 - val_accuracy: 0.1050\n",
            "Epoch 12/1000\n",
            "55/55 [==============================] - 0s 9ms/step - loss: 0.3065 - accuracy: 0.7573 - val_loss: 2.2057 - val_accuracy: 0.2215\n",
            "Epoch 13/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.2391 - accuracy: 0.8469 - val_loss: 1.3626 - val_accuracy: 0.4384\n",
            "Epoch 14/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.2658 - accuracy: 0.7818 - val_loss: 1.5558 - val_accuracy: 0.4543\n",
            "Epoch 15/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.2711 - accuracy: 0.8350 - val_loss: 0.5902 - val_accuracy: 0.7694\n",
            "Epoch 16/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.3022 - accuracy: 0.7858 - val_loss: 0.6392 - val_accuracy: 0.7466\n",
            "Epoch 17/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.5008 - accuracy: 0.7641 - val_loss: 2.5213 - val_accuracy: 0.7283\n",
            "Epoch 18/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.6680 - accuracy: 0.6602 - val_loss: 0.5870 - val_accuracy: 0.8288\n",
            "Epoch 19/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.3658 - accuracy: 0.7539 - val_loss: 3.8053 - val_accuracy: 0.2352\n",
            "Epoch 20/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.3062 - accuracy: 0.7687 - val_loss: 1.5690 - val_accuracy: 0.4110\n",
            "Epoch 21/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1951 - accuracy: 0.8521 - val_loss: 0.4663 - val_accuracy: 0.8242\n",
            "Epoch 22/1000\n",
            "55/55 [==============================] - 0s 9ms/step - loss: 0.1613 - accuracy: 0.8641 - val_loss: 0.4079 - val_accuracy: 0.8333\n",
            "Epoch 23/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1123 - accuracy: 0.9058 - val_loss: 0.2861 - val_accuracy: 0.8927\n",
            "Epoch 24/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1094 - accuracy: 0.9183 - val_loss: 0.4288 - val_accuracy: 0.8265\n",
            "Epoch 25/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1034 - accuracy: 0.9195 - val_loss: 0.3463 - val_accuracy: 0.8813\n",
            "Epoch 26/1000\n",
            "55/55 [==============================] - 0s 9ms/step - loss: 0.0784 - accuracy: 0.9343 - val_loss: 0.3179 - val_accuracy: 0.8813\n",
            "Epoch 27/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.0709 - accuracy: 0.9349 - val_loss: 0.3719 - val_accuracy: 0.8790\n",
            "Epoch 28/1000\n",
            "55/55 [==============================] - 0s 9ms/step - loss: 0.0569 - accuracy: 0.9566 - val_loss: 0.3705 - val_accuracy: 0.8767\n",
            "Epoch 29/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.0627 - accuracy: 0.9452 - val_loss: 0.3311 - val_accuracy: 0.8927\n",
            "Epoch 30/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.0707 - accuracy: 0.9372 - val_loss: 0.6047 - val_accuracy: 0.7991\n",
            "Epoch 31/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.0758 - accuracy: 0.9275 - val_loss: 0.3580 - val_accuracy: 0.8767\n",
            "Epoch 32/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1125 - accuracy: 0.9195 - val_loss: 0.6353 - val_accuracy: 0.8128\n",
            "Epoch 33/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1182 - accuracy: 0.9103 - val_loss: 0.4722 - val_accuracy: 0.8790\n",
            "Epoch 34/1000\n",
            "55/55 [==============================] - 0s 9ms/step - loss: 0.1353 - accuracy: 0.8938 - val_loss: 0.3495 - val_accuracy: 0.8927\n",
            "Epoch 35/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.0710 - accuracy: 0.9326 - val_loss: 0.5285 - val_accuracy: 0.8311\n",
            "Epoch 36/1000\n",
            "55/55 [==============================] - 0s 9ms/step - loss: 0.1065 - accuracy: 0.9160 - val_loss: 0.4851 - val_accuracy: 0.8493\n",
            "Epoch 37/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.2507 - accuracy: 0.8869 - val_loss: 2.8127 - val_accuracy: 0.4840\n",
            "Epoch 38/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.6447 - accuracy: 0.7133 - val_loss: 7.0810 - val_accuracy: 0.2443\n",
            "Epoch 39/1000\n",
            "55/55 [==============================] - 0s 9ms/step - loss: 0.5675 - accuracy: 0.6967 - val_loss: 0.4008 - val_accuracy: 0.9178\n",
            "Epoch 40/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.3329 - accuracy: 0.8184 - val_loss: 0.8683 - val_accuracy: 0.7374\n",
            "Epoch 41/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1859 - accuracy: 0.8407 - val_loss: 0.3141 - val_accuracy: 0.9064\n",
            "Epoch 42/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.0955 - accuracy: 0.9240 - val_loss: 0.3303 - val_accuracy: 0.8973\n",
            "Epoch 43/1000\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.0761 - accuracy: 0.9360 - val_loss: 0.5117 - val_accuracy: 0.8333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U8AISSQcaU1"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUzNVEJY33aI"
      },
      "source": [
        "def cls_report(path):\n",
        "    model = models.load_model(path)\n",
        "    pred = np.argmax(model.predict(x_val), axis=-1)\n",
        "    print(classification_report(np.argmax(y_val, axis=-1), pred))\n",
        "    print(confusion_matrix(np.argmax(y_val, axis=-1), pred))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk97-eaF5D-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59392a3c-9031-4d7a-8cce-2fce28cc294c"
      },
      "source": [
        "cls_report('best_balanced_model.h5')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94       369\n",
            "           1       0.26      0.60      0.36        10\n",
            "           2       0.61      0.77      0.68        22\n",
            "           3       0.22      0.50      0.31         4\n",
            "           4       0.94      0.91      0.92        33\n",
            "\n",
            "    accuracy                           0.89       438\n",
            "   macro avg       0.60      0.74      0.64       438\n",
            "weighted avg       0.93      0.89      0.91       438\n",
            "\n",
            "[[336  16   9   6   2]\n",
            " [  4   6   0   0   0]\n",
            " [  3   1  17   1   0]\n",
            " [  2   0   0   2   0]\n",
            " [  1   0   2   0  30]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}